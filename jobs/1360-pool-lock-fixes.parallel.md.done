---
title: Fix pool.go lock ordering: cancelFn under pm.mu deadlock, pool limit overrun in Release, ExcessRequeue nil startFn
created: 2026-02-21
origin: Post-audit Critical/High findings — C1 (nil startFn panic in ExcessRequeue), H3 (pool limit overrun in Release), H5 (cancelFn called with pm.mu held in tryDisplace), M4 (cancelFn called with pm.mu held in Dequeue)
priority: critical
complexity: high
notes:
  - Affects concurrent pool operations with potential for deadlocks
  - Hub.Broadcast acquires h.mu — calling cancelFn while holding pm.mu creates deadlock opportunity
  - Safe pattern exists in sweepExpired() and PurgeQueue() — collect under lock, release, then call callbacks
  - ExcessRequeue path currently enqueues items without startFn/cancelFn leading to nil pointer panic
  - Release() has a race window between dequeue and re-acquire that allows pool limit overrun
---

Follow exactly and without stopping:

## Task: Fix pool.go Lock Ordering Issues and Prevent Pool Limit Overrun

## Background & Context

During a post-audit security review of pool.go, four critical and high-priority issues were discovered related to lock ordering, deadlock potential, and pool invariant violations:

1. **Deadlock Risk (H5)**: In `tryDisplace()`, the displaced item's `cancelFn` is called while `pm.mu` is held. The `cancelFn` callback can call `hub.Broadcast()` which acquires `h.mu`, creating a nested lock acquisition that could cause deadlock if `hub` operations try to acquire `pm.mu`.

2. **Deadlock Risk (M4)**: In `Dequeue()`, `cancelFn` is called while `pm.mu` is held (via defer unlock), creating the same nested lock issue.

3. **Pool Limit Overrun (H3)**: In `Release()`, there is a race condition between dequeuing a running task, calling `startFn()` (outside lock), and re-adding to running. Another goroutine can fill the slot between these operations, causing the pool to exceed its limit.

4. **Nil Pointer Panic (C1)**: In `SetLimits()`, the `ExcessRequeue` path enqueues `QueuedItem` without setting `startFn` or `cancelFn`. When `Release()` later calls `next.startFn()`, it panics because `startFn` is nil.

The safe pattern for callback invocation already exists in `sweepExpired()` and `PurgeQueue()`: collect items/state under lock, release the lock, then invoke callbacks outside the critical section.

## Problem Description

### Issue H5: tryDisplace() — cancelFn Called Under pm.mu

File: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go
Location: Lines 397-453 (tryDisplace method)

Current behavior:
- `tryDisplace()` is called from `Acquire()` while `pm.mu` is held
- At line 442-443, `displaced.cancelFn()` is invoked while `pm.mu` is still held (because the lock was acquired at line 354 in `Acquire()` with defer at line 355)
- If `cancelFn` eventually calls `hub.Broadcast()`, which acquires `h.mu`, we have nested locks: `pm.mu` → `h.mu`
- Conversely, if the hub holds `h.mu` and tries to access pool state (acquiring `pm.mu`), we have: `h.mu` → `pm.mu` — potential deadlock

Specific symptom: No immediate error, but under load with certain timing, operations can hang waiting for locks.

### Issue M4: Dequeue() — cancelFn Called Under pm.mu

File: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go
Location: Lines 528-541 (Dequeue method)

Current behavior:
- `Dequeue()` acquires `pm.mu` at line 530
- `defer pm.mu.Unlock()` at line 531 releases lock after the function returns
- At line 534-535, `item.cancelFn()` is called while still holding `pm.mu`
- Same deadlock risk as H5

### Issue H3: Release() — Pool Limit Overrun Race

File: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go
Location: Lines 455-489 (Release method)

Current behavior:
- Line 457: `pm.mu.Lock()` — acquire lock
- Line 460-465: Remove released task from `pm.running`
- Line 468: `next := pm.nextQueued()` — dequeue next task
- Line 469: `pm.mu.Unlock()` — release lock
- Line 473: `next.startFn()` called outside lock
- Line 474: `pm.mu.Lock()` — re-acquire lock
- Line 475-480: Add to `pm.running`
- Line 481: `pm.mu.Unlock()`

Problem: Between line 469 and 474, another goroutine can call `Acquire()`, find capacity (because we haven't added the next task yet), and start a task. When this Release finishes adding the next task, we exceed the limit.

Example race:
1. Pool has limit 10, currently 9 running, 1 queued
2. Thread A: Release() removes one task → running = 8
3. Thread B: Acquire() checks `hasCapacity()` → true (8 < 10) → starts immediately → running = 9
4. Thread B: Acquire() re-adds the released task → running = 10 (at capacity, correct)
5. Thread A: Release() re-adds queued task → running = 11 (OVERRUN!)

### Issue C1: ExcessRequeue — nil startFn Panic

File: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go
Location: Lines 584-707 (SetLimits method)

Current behavior (lines 685-704):
- When `effectiveExcess.Action == ExcessRequeue`, excess running tasks are moved to the queue
- At lines 695-701, `QueuedItem` is enqueued with `startFn` and `cancelFn` set to nil (not provided)
- Later, when `Release()` dequeues and calls `next.startFn()` at line 473, `startFn` is nil → panic

The comment at line 700 says "startFn/cancelFn are not available here; caller must re-register" but this is unrealistic — the task is already running, we just need to track it.

## Implementation Plan

### Step 1: Fix H5 — tryDisplace() Return Displaced Item Instead of Calling cancelFn

1. Read the current tryDisplace() implementation (lines 397-453)
2. Change the signature to return the displaced item (or a slice of displaced items)
3. Instead of calling `displaced.cancelFn()` at line 443, collect and return the displaced item
4. In `Acquire()`, after releasing `pm.mu`, call `cancelFn` on any returned displaced items

New approach: Change return type from `bool` to `(bool, *QueuedItem)` to indicate success and the displaced item:

```go
func (pm *PoolManager) tryDisplace(action string, forced bool, newItem QueuedItem) (bool, *QueuedItem) {
    // ... find candidate ...
    
    // Displace and collect item (no callback)
    displaced := q.items[idx]
    q.items = append(q.items[:idx], q.items[idx+1:]...)
    q.mu.Unlock()
    
    // Enqueue new item
    newQ := pm.queueFor(newItem.action)
    newQ.Enqueue(newItem)
    pm.updateActionOrder(newItem.action)
    
    return true, &displaced  // caller will invoke cancelFn outside lock
}
```

### Step 2: Fix M4 — Dequeue() Release Lock Before Calling cancelFn

1. Read the current Dequeue() implementation (lines 528-541)
2. Replace the defer-based unlock with manual lock management
3. Collect the item, release pm.mu, then call cancelFn

```go
func (pm *PoolManager) Dequeue(taskID string) bool {
    pm.mu.Lock()
    var found *QueuedItem
    for _, q := range pm.queues {
        if foundFlag, item := q.Remove(taskID); foundFlag {
            found = &item
            break
        }
    }
    pm.mu.Unlock()  // release BEFORE calling cancelFn

    if found != nil && found.cancelFn != nil {
        found.cancelFn(errors.New("pool: task dequeued by request"))
    }
    return found != nil
}
```

### Step 3: Fix H3 — Release() Prevent Pool Limit Overrun via Atomic Registration

The issue is that Release() calls startFn() outside the lock, allowing another Acquire() to fill the slot.

Solution: Pre-register the next queued task in running WHILE HOLDING pm.mu, then call startFn outside. If startFn fails, roll back the running entry.

```go
func (pm *PoolManager) Release(taskID string) {
    pm.mu.Lock()

    // Remove released task from running
    for i, e := range pm.running {
        if e.taskID == taskID {
            pm.running = append(pm.running[:i], pm.running[i+1:]...)
            break
        }
    }

    // Get next queued task and PRE-REGISTER it as running
    // This prevents another Acquire() from filling the slot
    next := pm.nextQueued()
    if next != nil {
        // Pre-add to running (startFn may yet fail, but slot is reserved)
        pm.running = append(pm.running, RunningEntry{
            taskID:    next.taskID,
            action:    next.action,
            startedAt: time.Now(),
            forced:    next.forced,
            // stop field will be set if startFn succeeds; for now, nil is OK
        })
    }
    pm.mu.Unlock()

    // Call startFn outside the lock
    if next != nil {
        if err := next.startFn(); err != nil {
            // startFn failed — remove the pre-registered entry
            pm.mu.Lock()
            for i, r := range pm.running {
                if r.taskID == next.taskID {
                    pm.running = append(pm.running[:i], pm.running[i+1:]...)
                    break
                }
            }
            pm.mu.Unlock()

            // Notify caller
            if next.cancelFn != nil {
                next.cancelFn(err)
            }
        } else {
            // startFn succeeded; we're done (entry already added to running)
            // Note: the RunningEntry we added doesn't have stop set, which is acceptable
            // for requeued tasks coming from the pool queue.
        }
    }
}
```

### Step 4: Fix C1 — ExcessRequeue Nil startFn

The ExcessRequeue path (lines 668-673 and 685-704) tries to move running tasks back to the queue.

When these requeued items are later dequeued and startFn is called, startFn is nil (because we didn't provide one) → panic.

Solution: Log a warning and treat ExcessRequeue as ExcessStop for now. This prevents the nil panic while the feature is completed properly later.

In SetLimits(), change the loop at lines 667-673:

```go
for i := 0; i < excess_ && i < len(candidates); i++ {
    if effectiveExcess.Action == ExcessRequeue {
        // ExcessRequeue not yet properly implemented (would require storing startFn/stopFn in running entries).
        // For now, treat as stop to prevent nil startFn panic.
        toStop = append(toStop, candidates[i])
    } else {
        toStop = append(toStop, candidates[i])
    }
}
```

Then remove or simplify the requeue handling at lines 685-704:

```go
// Requeue is not yet implemented. The loop above now treats ExcessRequeue as ExcessStop.
// This section (old requeue handling) can be removed:
for _, e := range toRequeue {
    // REMOVED — now all excess tasks are stopped instead
}
```

## Technical Requirements

File paths:
- /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go

Dependencies:
- Standard library: errors, sync, time, sort (already imported)

Commands to use:
- `go test -race ./...` to test concurrency
- `go vet ./...` for static analysis
- `make build && ./dist/sticky-overseer` to verify no panics on startup

Reference implementations:
- sweepExpired() (lines 798-816) — shows safe pattern: collect under lock, release, call callbacks
- PurgeQueue() (lines 543-582) — shows safe pattern
- Dequeue() in TaskQueue (lines 119-149) — already does lazy expiry outside lock

## Changes Required

### Change 1: Modify tryDisplace() signature and implementation

- Change return type from `bool` to `(bool, *QueuedItem)`
- Remove the cancelFn call (line 443)
- Return the displaced item instead: `return true, &displaced`
- Return `false, nil` instead of `false`

### Change 2: Update Acquire() to handle returned displaced item

- Capture the return value: `ok, displaced := pm.tryDisplace(...)`
- After pm.mu is released (via defer), check if displaced is not nil and call cancelFn on it
- This requires collecting the displaced item and calling it outside the lock in Acquire

### Change 3: Fix Dequeue() to release lock before calling cancelFn

- Remove `defer pm.mu.Unlock()` at line 531
- Replace with manual lock management: acquire, collect item, release, then call cancelFn
- Ensure function returns the correct bool

### Change 4: Refactor Release() to pre-register next queued task

- Pre-add the next queued task to pm.running WHILE holding pm.mu
- Call startFn outside the lock
- If startFn fails, roll back the pre-added entry
- If startFn succeeds, the entry remains in running

### Change 5: Handle ExcessRequeue as ExcessStop (temporary workaround)

- In SetLimits(), update the loop at lines 667-673 to treat ExcessRequeue the same as ExcessStop
- Remove or comment out the toRequeue handling section (lines 685-704)

## Success Criteria

1. No nil pointer panic when ExcessRequeue is configured and limit reduction occurs
2. No deadlock hangs under concurrent Acquire/Release/hub.Broadcast operations (test with race detector)
3. Pool.running never exceeds the configured limit except in the initial brief window before pre-added entries are validated
4. cancelFn callbacks are never invoked while pm.mu is held
5. All existing tests pass with `go test -race ./...`
6. Build succeeds with no new warnings: `make build`

## Expected Outcome

- pool.go has four lock ordering issues fixed
- ExcessRequeue temporarily disabled with safe fallback to stop (to be properly implemented later with startFn/stopFn storage)
- No more potential deadlocks between pm.mu and h.mu
- Pool limit enforced correctly without race conditions
- No nil pointer panics in Release()

## Reference Information

- Pool architecture: hub.go manages Hub, which calls pool methods; Hub has h.mu lock
- Broadcast calls: hub.go Broadcast() method (likely acquires h.mu)
- Lock hierarchy: should be h.mu > pm.mu (hub locks before pool), never pm.mu > h.mu
- Safe callback pattern: sweepExpired() at lines 798-816 shows collect-under-lock, call-under-no-lock
- Current pool.go: 817 lines total

## Notes & Warnings

- Be careful with the order of changes: fix Dequeue() and tryDisplace() first (they're independent), then Release(), then SetLimits()
- Test with `go test -race ./...` after each change to catch race condition issues
- The pre-registration approach in Release() introduces a small window where an entry is in running but startFn hasn't been called yet. This is acceptable and safer than the current race condition.
- ExcessRequeue is a feature stub; properly implementing it would require storing the original startFn/stopFn in RunningEntry or having a different requeue mechanism (e.g., dequeue and re-Acquire with original context).
- After fixes, verify that the Acquire() changes properly handle the new return type from tryDisplace() and that the displaced item's cancelFn is called outside pm.mu
