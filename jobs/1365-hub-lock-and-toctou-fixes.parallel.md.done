---
title: Fix hub.go lock ordering (startFn under pm.mu) and TOCTOU duplicate task race
created: 2026-02-21
origin: Post-audit Critical findings C2/C3 (startFn called inside pm.mu, which acquires h.mu — lock inversion risk) and High finding H2 (duplicate task TOCTOU — two concurrent starts can both pass the duplicate check).
priority: critical
complexity: high
notes:
  - PREREQUISITE: Job 1350 (hub-cleanup-db-removal) must be completed first
  - After DB removal, startFn only performs in-memory operations (register in h.tasks, call handler.Start)
  - Lock inversion path: pm.mu (Acquire) → startFn → h.mu (register task) AND h.mu.RLock (handleList) → pool.Info() → pm.mu (deadlock)
  - TOCTOU window: duplicate check (RLock/RUnlock) → pool.Acquire → startFn (Lock/register) — two goroutines can both pass check
  - pool.Release() already correctly releases pm.mu before calling startFn — follow the same pattern
  - After fix, must run "go build ./..." and "go test -race ./..." with zero race detector hits
---

Follow exactly and without stopping:

## Task: Fix hub.go lock ordering (Critical C2/C3) and TOCTOU duplicate task race (High H2)

## Background & Context

During security audit (post-job 1330), two critical concurrency issues were identified in hub.go and pool.go:

1. **Critical C2/C3 — Lock Inversion Risk**: startFn is called inside pool.Acquire while pm.mu is held. startFn acquires h.mu to register the task. Separately, handleList holds h.mu.RLock and calls pool.Info() which tries to acquire pm.mu. This creates a deadlock cycle: pm.mu → h.mu (in startFn) and h.mu.RLock → pm.mu (in pool.Info). Lock ordering violation occurs whenever pm.mu is held while acquiring or waiting for h.mu.

2. **High H2 — TOCTOU Duplicate Task Race**: handleStart checks if a task is already running (RLock/RUnlock at lines 383–385), then releases h.mu before calling pool.Acquire. Between the check and pool.Acquire/startFn, a second concurrent handleStart for the same taskID can pass the duplicate check. Both goroutines then reach startFn and both register h.tasks[taskID], with the second overwriting the first. This violates the invariant that only one worker runs per taskID at a time.

## Problem Description

**Symptoms of C2/C3 (Lock Inversion)**:
- Occasional deadlocks when pool.Acquire and handleList run concurrently
- Process hangs with no goroutine making progress on both h.mu and pm.mu
- Error manifests when: a client calls /ws start (acquires pool, calls startFn which waits for h.mu) while another connection calls list (holds h.mu.RLock, tries pool.Info which needs pm.mu)

**Symptoms of H2 (TOCTOU Duplicate Task)**:
- Two identical-taskID start messages sent in quick succession may both succeed
- Second start overwrites the first task's entry in h.tasks
- First worker object is orphaned; only one worker is tracked but two processes may run
- Worker exit callbacks may target wrong task or fail to find task

**Example Race Window for H2**:
```
Goroutine A (handleStart, taskID="task1"):
1. h.mu.RLock() [line 383]
2. _, exists := h.tasks["task1"]  (does not exist)
3. h.mu.RUnlock() [line 385]
   [WINDOW OPENS — another goroutine can run]

Goroutine B (handleStart, taskID="task1"):
1. h.mu.RLock() [line 383]
2. _, exists := h.tasks["task1"]  (still does not exist)
3. h.mu.RUnlock() [line 385]
   [WINDOW CLOSES — both A and B proceed]

Both A and B:
4. Call pool.Acquire → startFn
5. In startFn, both register h.tasks["task1"] = task
6. First task's worker is lost; second task overwrites it
```

## Root Cause Analysis

**C2/C3 Lock Inversion**:
- pool.Acquire (line 354) acquires pm.mu with defer unlock (line 355)
- startFn is called at line 359 WHILE pm.mu is still held
- startFn (hub.go line 404) eventually calls h.mu.Lock at line 425 to register the task
- This creates pm.mu → h.mu ordering
- Separately, handleList (line ~250) holds h.mu.RLock and calls pool.Info() (line ~260)
- pool.Info() (pool.go line ~330) acquires pm.mu under the h.mu.RLock
- This creates h.mu.RLock → pm.mu ordering
- These two orderings form a cycle: DEADLOCK

**H2 TOCTOU Race**:
- Duplicate check at handleStart lines 383–394 uses RLock/RUnlock (not atomic with pool.Acquire call)
- Any code between RUnlock and pool.Acquire/startFn completion is a race window
- pool.Acquire synchronously calls startFn, which registers the task, but the check-then-act is not atomic
- Solution: atomically reserve the taskID BEFORE releasing h.mu, using a pending set

## Implementation Plan

### Task 1: Fix C2/C3 — Restructure pool.Acquire to call startFn outside pm.mu

**File to modify**: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go

**Current state** (lines 347–395):
- pm.mu.Lock() (line 354)
- defer pm.mu.Unlock() (line 355) — held for entire function
- hasCapacity check (line 357)
- startFn() called at line 359 WHILE pm.mu held (PROBLEM)
- running entry appended (lines 362–368)

**New structure**:
```go
func (pm *PoolManager) Acquire(
	taskID, action string,
	forced bool,
	stopFn func(),
	startFn func() error,
	cancelFn func(reason error),
) (AcquireResult, error) {
	pm.mu.Lock()
	
	if pm.hasCapacity(action) {
		// Pre-register as running UNDER the lock (also prevents H3 overrun).
		pm.running = append(pm.running, RunningEntry{
			taskID:    taskID,
			action:    action,
			startedAt: time.Now(),
			forced:    forced,
			stop:      stopFn,
		})
		pm.mu.Unlock()
		
		// Call startFn OUTSIDE pm.mu — can safely acquire h.mu without deadlock risk.
		if err := startFn(); err != nil {
			// Start failed — remove pre-registration.
			pm.mu.Lock()
			for i, r := range pm.running {
				if r.taskID == taskID {
					pm.running = append(pm.running[:i], pm.running[i+1:]...)
					break
				}
			}
			pm.mu.Unlock()
			return AcquireRunning, err
		}
		return AcquireRunning, nil
	}
	
	// Queue path — unchanged.
	q := pm.queueFor(action)
	item := QueuedItem{
		taskID:     taskID,
		action:     action,
		enqueuedAt: time.Now(),
		forced:     forced,
		startFn:    startFn,
		cancelFn:   cancelFn,
	}
	
	if q.cfg.Enabled {
		if q.Enqueue(item) {
			pm.updateActionOrder(action)
			pm.mu.Unlock()
			return AcquireQueued, nil
		}
		// Queue full — try displacement.
		if pm.tryDisplace(action, forced, item) {
			pm.mu.Unlock()
			return AcquireQueued, nil
		}
	}
	
	pm.mu.Unlock()
	return AcquireRunning, errPoolRejected
}
```

**Key changes**:
- After hasCapacity check, immediately append to pm.running WHILE holding pm.mu (prevents capacity overrun)
- Release pm.mu with explicit unlock() instead of defer, BEFORE calling startFn
- If startFn fails, re-acquire pm.mu and remove the pre-registered running entry
- Queue path and tryDisplace path now explicitly unlock before returning (not deferred)
- This ensures startFn can safely acquire h.mu without pm.mu held

**Verification**:
- Compile with "go build ./..."
- Run "go test -race ./..." — should have zero race detector hits
- Manual test: start task, call list simultaneously → no deadlock

---

### Task 2: Fix H2 — Add pending set to atomically reserve taskID during start

**File to modify**: /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/hub.go

**Step 2A: Add pending map to Hub struct** (around line 42, after the Hub struct definition)

Current state (lines 42–53):
```go
type Hub struct {
	mu           sync.RWMutex
	clients      map[Conn]struct{}        // all connected clients
	tasks        map[string]*Task         // task_id → Task
	db           *sql.DB
	actions      map[string]ActionHandler // immutable after startup
	pool         *PoolManager             // manages concurrency + queue
	eventLog     *json.Encoder
	eventLogMu   sync.Mutex // serialises concurrent Encode calls on eventLog
	shutdownCh   chan struct{}
	shutdownOnce sync.Once
}
```

Add after line 45 (after tasks field):
```go
	pending      map[string]struct{}      // task IDs currently being started; protected by h.mu
```

**Step 2B: Initialize pending in newHub** (around line 56)

Current newHub function creates h:
```go
h := &Hub{
	clients:    make(map[Conn]struct{}),
	tasks:      make(map[string]*Task),
	...
}
```

Add pending initialization:
```go
	pending:    make(map[string]struct{}),
```

**Step 2C: Modify handleStart duplicate check** (lines 382–394)

Current code:
```go
	// Check for a duplicate task that is already running.
	h.mu.RLock()
	existingTask, exists := h.tasks[taskID]
	h.mu.RUnlock()
	if exists {
		existingTask.mu.Lock()
		running := existingTask.worker != nil && existingTask.worker.State == WorkerRunning
		existingTask.mu.Unlock()
		if running {
			h.sendError(conn, msg.ID, "task already has a running worker")
			return
		}
	}
```

Replace with:
```go
	// Atomically check for duplicates and reserve the taskID.
	h.mu.Lock()
	existingTask, exists := h.tasks[taskID]
	_, isPending := h.pending[taskID]
	if exists || isPending {
		h.mu.Unlock()
		h.sendError(conn, msg.ID, "task already running or starting")
		return
	}
	// Reserve the taskID to prevent concurrent start attempts.
	h.pending[taskID] = struct{}{}
	h.mu.Unlock()
```

**Rationale**: 
- Convert from RLock to full Lock to ensure atomicity
- Check both h.tasks (already running) and h.pending (currently starting)
- Reserve taskID in pending BEFORE releasing h.mu
- This atomically prevents any concurrent goroutine from also reserving the same taskID

**Step 2D: Update startFn to remove from pending and register in tasks** (lines 404–450)

Current startFn (lines 404–450 approximately):
```go
	startFn := func() error {
		now := time.Now().UTC()
		rec := TaskRecord{
			TaskID:        taskID,
			Action:        action,
			Params:        params,
			RetryPolicy:   retryPolicy,
			State:         StateActive,
			CreatedAt:     now,
			LastStartedAt: &now,
		}

		if h.db != nil {
			if err := createTask(h.db, rec); err != nil {
				return fmt.Errorf("db error: %w", err)
			}
		}

		// Register the task BEFORE starting the worker so onWorkerExited
		// can always find it, even if the process exits immediately.
		task := &Task{record: rec}
		h.mu.Lock()
		h.tasks[taskID] = task
		h.mu.Unlock()

		w, err := handler.Start(taskID, params, h.workerCB())
		if err != nil {
			...
		}
		...
	}
```

Replace the h.mu.Lock/Unlock section (around lines 425–427) with:
```go
		// Remove from pending and register in tasks.
		task := &Task{record: rec}
		h.mu.Lock()
		delete(h.pending, taskID)  // remove from pending
		h.tasks[taskID] = task     // register in active tasks
		h.mu.Unlock()
```

**Rationale**: 
- After DB operations (which now don't exist after job 1350), atomically transition from pending→active
- Delete from h.pending to signal that task is no longer "starting"
- Register in h.tasks where it can be found by pool callbacks and other handlers

**Step 2E: Add cleanup in startFn error path** (around line 450)

Current code (approx lines 430–450) has error handling after handler.Start. Add cleanup for startFn errors:

After the handler.Start error block, if the task cannot be started, ensure cleanup:
```go
		w, err := handler.Start(taskID, params, h.workerCB())
		if err != nil {
			// Start failed — remove from active tasks and pending.
			h.mu.Lock()
			delete(h.tasks, taskID)
			delete(h.pending, taskID)
			h.mu.Unlock()
			return err
		}
```

**Rationale**: 
- If handler.Start fails, the task was already registered in h.tasks
- Must clean up both h.tasks and h.pending to fully revert the reservation
- This allows the client or a retry to start the same taskID again

**Step 2F: Update cancelFn for queued tasks** (around line 470)

Current cancelFn:
```go
	cancelFn := func(reason error) {
		...
		// broadcast dequeued
	}
```

Add cleanup at the start:
```go
	cancelFn := func(reason error) {
		// Clean up pending reservation if task was queued before starting.
		h.mu.Lock()
		delete(h.pending, taskID)
		h.mu.Unlock()
		
		// ... rest of cancelFn (broadcast dequeued, etc.)
	}
```

**Rationale**: 
- If a task is queued and then cancelled (e.g., list is cleared, or queue is full and not displaced), it was never started
- Must still remove from h.pending to allow the taskID to be reused
- This prevents orphaned reservations that block future starts

---

### Task 3: Verify and test

**Step 3A: Build**
```bash
cd /home/mmulligan/Development/whisper-darkly-github/sticky-overseer
go build ./...
```
Expected result: No build errors.

**Step 3B: Run race detector**
```bash
go test -race ./...
```
Expected result: All tests pass, zero race detector warnings. If races are detected:
- Identify the race in the output (file:line)
- Review the lock acquired during the race
- Ensure h.mu is held for the entire critical section, or pm.mu is released before acquiring h.mu

**Step 3C: Manual concurrency test (optional but recommended)**

Create a simple test scenario:
1. Start a long-running task (e.g., "sleep 60")
2. While it runs, immediately call /ws start with the SAME taskID twice in parallel
3. Expected: Only one worker should run; the second attempt should receive "task already running or starting" error
4. Call /ws list repeatedly while task is starting
5. Expected: No deadlock; list always responds immediately

Example with curl:
```bash
# Terminal 1: Start overseer
./dist/sticky-overseer

# Terminal 2: Start first task and second task in parallel
curl -i -N -H "Connection: Upgrade" -H "Upgrade: websocket" \
  ws://localhost:8080/ws 2>&1 | grep -i "upgrade\|switching" &

# Send: {"type":"start","task_id":"test1","action":"exec","params":{"cmd":"sleep","args":["60"]}}
# Then immediately send again from another connection with same task_id
```

**Step 3D: Verify no regression in existing tests**
```bash
go test -v ./...
```
All tests should pass. If a test fails:
- Review the test to understand what it checks
- Ensure the fix doesn't change the expected behavior
- Adjust test if necessary (with clear comments on why)

---

## Technical Requirements

**Files to modify**:
- /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go (Acquire method, lines 347–395)
- /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/hub.go (Hub struct, newHub, handleStart, startFn, cancelFn)

**Dependencies**:
- sync package (already imported)
- time package (already imported)
- Job 1350 (hub-cleanup-db-removal) must be completed first for startFn to avoid I/O

**Commands**:
- go build ./...
- go test -race ./...
- go test -v ./...

**Testing environment**:
- Linux or macOS with Go 1.16+
- Sufficient CPU to run -race tests (may be slower)

---

## Success Criteria

1. **C2/C3 Fix (Lock Ordering)**:
   - pool.Acquire releases pm.mu before calling startFn ✓
   - startFn can acquire h.mu without pm.mu held ✓
   - handleList can acquire h.mu.RLock and call pool.Info without deadlock ✓
   - "go build ./..." passes with no errors ✓
   - "go test -race ./..." passes with zero race detector hits ✓

2. **H2 Fix (TOCTOU Duplicate Task)**:
   - h.pending map added to Hub struct and initialized ✓
   - handleStart atomically reserves taskID in h.pending under h.mu.Lock ✓
   - Two concurrent starts with the same taskID: first succeeds, second gets "already running or starting" ✓
   - startFn removes taskID from h.pending and registers in h.tasks ✓
   - cancelFn removes taskID from h.pending for queued (unstarted) tasks ✓
   - startFn error path cleans up both h.tasks and h.pending ✓

3. **Integration**:
   - All existing tests pass ✓
   - No new race detector warnings in test suite ✓
   - Manual concurrency test shows no duplicate task runs ✓
   - No regressions in /ws list, /ws stop, or other handlers ✓

---

## Expected Outcome

After completing this job:
- Lock inversion deadlock between pool.Acquire and handleList is eliminated
- TOCTOU race in duplicate task check is closed; taskID is atomically reserved
- No race detector warnings from "-race" test flag
- Pool capacity and task state are always consistent
- Security audit findings C2, C3, and H2 are resolved

---

## Reference Information

**Files involved**:
- /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/hub.go (Hub struct, handleStart, startFn, cancelFn)
- /home/mmulligan/Development/whisper-darkly-github/sticky-overseer/pool.go (Acquire method, tryDisplace)

**Related jobs**:
- Job 1350 (hub-cleanup-db-removal) — removes DB I/O from startFn, simplifying the lock fix
- Job 1360 (pool-displacement-displacement-policy) — improves tryDisplace handling

**Lock dependency graph** (what must NOT occur):
```
BEFORE (DEADLOCK PATH):
Thread A: pm.mu → startFn → h.mu     [in Acquire line 359]
Thread B: h.mu.RLock → pool.Info() → pm.mu  [in handleList]
Result: CYCLE — DEADLOCK

AFTER (FIXED):
Thread A: pm.mu (released) → startFn → h.mu  [no pm.mu held in startFn]
Thread B: h.mu.RLock → pool.Info() → pm.mu [OK; no conflicting pm.mu holder]
Result: NO DEADLOCK
```

**Audit findings**:
- C2: "startFn is called inside pm.Acquire while pm.mu is held"
- C3: "handleList holds h.mu.RLock and calls pool.Info() which acquires pm.mu — lock inversion with startFn"
- H2: "duplicate task TOCTOU — two concurrent starts can both pass the duplicate check before either registers in h.tasks"

---

## Notes & Warnings

**Critical sequencing**:
- Job 1350 (hub-cleanup-db-removal) MUST be completed before this job, as it simplifies startFn
- Do NOT merge this job without job 1350 already in main, or startFn will still do I/O under the wrong lock

**Lock ordering rules** (enforce globally):
- NEVER call a function that needs h.mu while holding pm.mu
- NEVER call a function that needs pm.mu while holding h.mu (RLock or Lock)
- If a function acquires h.mu, do not acquire pm.mu inside it

**TOCTOU atomicity**:
- The pending set MUST be checked and reserved in the same h.mu.Lock block
- Any code path that calls pool.Acquire must first reserve the taskID in h.pending
- Cleanup (delete from pending) must occur in startFn, cancelFn, and error paths

**Testing with -race flag**:
- Go's -race detector runs single-threaded with heavy instrumentation (slow)
- Run "go test -race ./..." on a quiet system for best results
- If a race is reported but looks spurious, add comments near the locks explaining the invariant

**Future maintenance**:
- Any new code that accesses h.tasks must hold h.mu
- Any new code that accesses h.pending must hold h.mu
- Review new Acquire paths to ensure startFn is called outside pm.mu
- If handleList or similar functions call pool methods, verify no lock cycles exist

